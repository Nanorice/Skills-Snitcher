{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House-keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:24:57.087029Z",
     "start_time": "2020-08-26T11:24:56.548308Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T16:46:48.710712Z",
     "start_time": "2020-08-25T16:46:48.708867Z"
    }
   },
   "source": [
    "# Handy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:25:07.735347Z",
     "start_time": "2020-08-26T11:25:07.732277Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def allDone():\n",
    "    '''this function outputs a short audio when called. \n",
    "    Typically this is used to signal a task completion'''\n",
    "    \n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect job postings with predefined category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:26:45.872293Z",
     "start_time": "2020-08-26T11:26:45.860279Z"
    },
    "code_folding": [
     0,
     40,
     46,
     52
    ]
   },
   "outputs": [],
   "source": [
    "def indeedScrape(tSearch = 'data scientist', nMax = 50):\n",
    "    \n",
    "    columns = ['location', 'company_name', 'job_title', 'summary', 'full_info', 'ref']\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "\n",
    "    metaUrl = 'https://www.indeed.co.uk/jobs?q=%(search)s&l=United+Kingdom&radius=100&start=' % {'search':tSearch.replace(' ', '+')}\n",
    "\n",
    "    for start in range(0, nMax, 10):\n",
    "        \n",
    "        try:\n",
    "            url = metaUrl + str(start)\n",
    "            page = requests.get(url)\n",
    "#             print('retriving url: ', url)\n",
    "        except:\n",
    "            print('---Failed to retrieve---')\n",
    "            print('url:', url)\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#         time.sleep(1)\n",
    "        \n",
    "        ## metadata from mainpage\n",
    "        # extract info from class:row\n",
    "        # company name and job title\n",
    "        companies, jobs = [], []\n",
    "        for div in soup.find_all(name = 'div', attrs = {'class':'row'}):\n",
    "            company = div.find_all(name = 'span', attrs = {'class':'company'})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                    companies.append(b.text.strip())\n",
    "                else:\n",
    "                    sec_try = div.find_all(name = 'span', attrs = {'class':'result-link-source'})\n",
    "                    for span in sec_try:\n",
    "                        companies.append(span.text.strip())\n",
    "            for a in div.find_all(name = 'a', attrs = {'data-tn-element':\"jobTitle\"}):\n",
    "                jobs.append(a['title'])\n",
    "\n",
    "        # extract location\n",
    "        locations = []\n",
    "        spans = soup.find_all('span', attrs={'class':'location'})\n",
    "        for span in spans:\n",
    "            locations.append(span.text)\n",
    "\n",
    "        # extract summaries\n",
    "        summaries = []\n",
    "        divs = soup.find_all('div', attrs = {'class':'summary'})\n",
    "        for i, div in enumerate(divs):\n",
    "            summaries.append(div.text.strip())\n",
    "\n",
    "        ## crawl to subpages\n",
    "        descriptions = []\n",
    "        link_list = []\n",
    "        for adlink in soup.select('a[onmousedown*=\"return rclk(this,jobmap[\"]'):\n",
    "            suburl = \"https://www.indeed.com\" + adlink['href']\n",
    "            link_list.append(suburl)\n",
    "            try:\n",
    "                subpage = requests.get(suburl)\n",
    "                subsoup = BeautifulSoup(subpage.text)\n",
    "            except:\n",
    "                print('--- Failed to retrieve sub-URL ---')\n",
    "                print('url: ', suburl)\n",
    "            # extract descriptions\n",
    "            for des in subsoup.select('div[class*=\"jobsearch-JobComponent-description\"]'): \n",
    "                 descriptions.append(des.get_text())\n",
    "\n",
    "        df_temp = list(zip(locations, companies, jobs, summaries, descriptions, link_list))\n",
    "        df_temp = pd.DataFrame(df_temp, columns = columns)\n",
    "        df = df.append(df_temp).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:26:46.111757Z",
     "start_time": "2020-08-26T11:26:46.109417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Financial part - Done!\n",
    "# # 13-2099.01\n",
    "# Financial_Quantitative_Analysts = indeedScrape(tSearch='Financial Quantitative Analysts', nMax=300)\n",
    "# Financial_Quantitative_Analysts['soc'] = '13-2099.01'\n",
    "# # 13-2051.00\n",
    "# Financial_Analysts = indeedScrape(tSearch='Financial Analysts', nMax=1000)\n",
    "# Financial_Analysts['soc'] = '13-2051.00'\n",
    "# # 13-2052.00\n",
    "# Financial_Advisors = indeedScrape(tSearch='Financial Advisors', nMax=2000)\n",
    "# Financial_Advisors['soc'] = '13-2052.00'\n",
    "\n",
    "# df = pd.concat([Financial_Analysts, Financial_Advisors, Financial_Quantitative_Analysts], ignore_index=True)\n",
    "# dirPData = '../data/'\n",
    "# f_name = dirPData + 'financial_jobs_with_soc.pickle'\n",
    "# with open(f_name, \"wb\") as f:\n",
    "#     pickle.dump(df, f)\n",
    "    \n",
    "# allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T11:58:18.869626Z",
     "start_time": "2020-08-26T11:31:33.980667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Telecommunication\n",
    "# 17-2071.00\n",
    "Electrical_Engineers = indeedScrape(tSearch='Electrical Engineers', nMax = 1000)\n",
    "Electrical_Engineers['soc'] = '17-2071.00'\n",
    "# 17-2141.00\n",
    "Mechanical_Engineers = indeedScrape(tSearch='Mechanical Engineers', nMax = 1000)\n",
    "Mechanical_Engineers['soc'] = '17-2141.00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T12:32:31.653650Z",
     "start_time": "2020-08-26T12:32:31.635201Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([Electrical_Engineers, Mechanical_Engineers], ignore_index=True)\n",
    "dirPData = '../data/'\n",
    "f_name = dirPData + 'telecom_jobs_with_soc.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
