{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:41:00.952824Z",
     "start_time": "2020-06-29T13:41:00.949469Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:41:01.347618Z",
     "start_time": "2020-06-29T13:41:01.338024Z"
    }
   },
   "outputs": [],
   "source": [
    "# def extract_salary(soup):\n",
    "#     salaries = []\n",
    "#     for div in soup.find_all(name = 'div', attrs = {'class':'row'}):\n",
    "#         try:\n",
    "#             salaries.append(div.find('nobr').text)\n",
    "#         except:\n",
    "#             try:\n",
    "#                 div_two = div.find(name = 'div', attrs = {'class':'sjcl'})\n",
    "#                 div_three = div_two.find('div')\n",
    "#                 salaries.append(div_three.text.strip())\n",
    "# #                 print(div_three)\n",
    "#             except:\n",
    "#                 salaries.append('Nothing_found')    \n",
    "#     return(salaries)\n",
    "\n",
    "def extract_salary(soup):\n",
    "    salary = []\n",
    "    for div in soup.find_all(name='div', attrs = {'class':'row'}):\n",
    "        if div.find(name = 'span', attrs = {'class':\"salaryText\"}):\n",
    "            span = div.find(name = 'span', attrs = {'class':\"salaryText\"})\n",
    "            salary.append(span.text)\n",
    "        else:\n",
    "            salary.append(np.NaN)\n",
    "    return salary\n",
    "\n",
    "def extract_summary(soup):\n",
    "    summaries = []\n",
    "    divs = soup.find_all('div', attrs = {'class':'summary'})\n",
    "#     print(divs)\n",
    "    for i, div in enumerate(divs):\n",
    "#         print(i, div.text)\n",
    "        summaries.append(div.text.strip())\n",
    "    return(summaries)\n",
    "\n",
    "def extract_location(soup):\n",
    "    locations = []\n",
    "    spans = soup.find_all('span', attrs={'class':'location'})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    return(locations)\n",
    "\n",
    "def extract_company(soup):\n",
    "    companies = []\n",
    "    for div in soup.find_all(name = 'div', attrs = {'class':'row'}):\n",
    "        company = div.find_all(name = 'span', attrs = {'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name = 'span', attrs = {'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    companies.append(span.text.strip())\n",
    "    return(companies)\n",
    "\n",
    "def extract_job_title(soup):\n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs = {'class':'row'}):\n",
    "        for a in div.find_all(name = 'a', attrs = {'data-tn-element':\"jobTitle\"}):\n",
    "            jobs.append(a['title'])\n",
    "    return(jobs)\n",
    "\n",
    "def extract_post_date(soup):\n",
    "    dates = []\n",
    "    for div in soup.find_all(name='div', attrs = {'class':'row'}):\n",
    "        for span in div.find_all(name = 'span', attrs = {'class':\"date\"}):\n",
    "            dates.append(span.text)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:41:03.555325Z",
     "start_time": "2020-06-29T13:41:03.123120Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL = 'https://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10'\n",
    "# URL = 'https://www.indeed.co.uk/jobs?q=quantitative+analyst+%C2%A340,000&l=London'\n",
    "URL = 'https://www.indeed.co.uk/jobs?q=Finance&l=London&radius=100' # finance job in London\n",
    "#conducting a request of the stated URL above:\n",
    "page = requests.get(URL)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:41:40.526996Z",
     "start_time": "2020-06-29T13:41:40.178566Z"
    },
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "URL = 'https://www.indeed.co.uk/jobs?q=quantitative+analyst+%C2%A340,000&l=London'\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "title = extract_job_title(soup)\n",
    "\n",
    "company = extract_company(soup)\n",
    "\n",
    "ls = extract_location(soup)\n",
    "\n",
    "summary = extract_summary(soup)\n",
    "    \n",
    "salary = extract_salary(soup)\n",
    "\n",
    "# date = extract_post_date(soup)\n",
    "\n",
    "# df1 = zip(company, title, ls, salary, summary)\n",
    "# df = pd.DataFrame(df1, columns = ['company', \"title\", 'location', 'salary', 'summary'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:42:53.937031Z",
     "start_time": "2020-06-29T13:42:53.932954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selby Jennings',\n",
       " 'Spencer Clarke Group',\n",
       " 'Selby Jennings',\n",
       " 'IHS Markit',\n",
       " 'NHS England and NHS Improvement',\n",
       " 'Google',\n",
       " 'Deutsche Bank',\n",
       " 'Ipreo',\n",
       " 'CITI',\n",
       " 'CITI',\n",
       " 'CITI',\n",
       " 'Deutsche Bank',\n",
       " 'UK Government - Office for National Statistics']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:43:14.400353Z",
     "start_time": "2020-06-29T13:43:14.393185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Selby Jennings': 'FO Quantitative Analyst - Pricing',\n",
       " 'Spencer Clarke Group': 'Behavioral Insight Analyst',\n",
       " 'IHS Markit': 'Quantitative Analyst, iBoxx Indices',\n",
       " 'NHS England and NHS Improvement': 'Analyst',\n",
       " 'Google': 'Financial Analyst, Hardware',\n",
       " 'Deutsche Bank': 'Quantitative Analyst',\n",
       " 'Ipreo': 'Quantitative Analyst, iBoxx Indices',\n",
       " 'CITI': 'XVA Quantitative Analyst',\n",
       " 'UK Government - Office for National Statistics': 'Methods, Data and Research Analyst'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(company, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:43:49.805265Z",
     "start_time": "2020-06-29T13:43:49.795853Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data argument can't be an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4f9c1d2b5ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df1 = dict(zip(company, title))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data argument can't be an iterator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data argument can't be an iterator"
     ]
    }
   ],
   "source": [
    "# df1 = dict(zip(company, title))\n",
    "df1 = zip(company, title)\n",
    "pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:37:52.696630Z",
     "start_time": "2020-06-29T13:37:52.285462Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving url:  https://www.indeed.co.uk/jobs?q=finance&l=London&radius=25&start=0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data argument can't be an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c7c2ebe7b6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data argument can't be an iterator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data argument can't be an iterator"
     ]
    }
   ],
   "source": [
    "## fetching metadata\n",
    "max_jobs_to_fetch = 1000\n",
    "columns = ['location', 'company_name', 'job_title', 'summary']\n",
    "# columns = ['company', 'title']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "for start in range(0, max_jobs_to_fetch, 10):\n",
    "    try:\n",
    "        url = 'https://www.indeed.co.uk/jobs?q=finance&l=London&radius=25&start='\n",
    "        page = requests.get(url + str(start))\n",
    "        print('retriving url: ', (url + str(start)))\n",
    "    except:\n",
    "        print('---Failed to retrieve---')\n",
    "        print('url:', (url + str(start)))\n",
    "        continue\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    # extract info from class:row\n",
    "    # company name and job title\n",
    "    companies, jobs = [], []\n",
    "    for div in soup.find_all(name = 'div', attrs = {'class':'row'}):\n",
    "        company = div.find_all(name = 'span', attrs = {'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name = 'span', attrs = {'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    companies.append(span.text.strip())\n",
    "        for a in div.find_all(name = 'a', attrs = {'data-tn-element':\"jobTitle\"}):\n",
    "            jobs.append(a['title'])\n",
    "            \n",
    "    # extract location\n",
    "    locations = []\n",
    "    spans = soup.find_all('span', attrs={'class':'location'})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "        \n",
    "    # extract summaries\n",
    "    summaries = []\n",
    "    divs = soup.find_all('div', attrs = {'class':'summary'})\n",
    "    for i, div in enumerate(divs):\n",
    "        summaries.append(div.text.strip())\n",
    "        \n",
    "    df_temp = zip(locations, companies, jobs, summaries)\n",
    "    df_temp = pd.DataFrame(df_temp, columns = columns)\n",
    "    df = df.append(df_temp).reset_index(drop = True)\n",
    "    \n",
    "df.to_csv('sample_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:40:36.424947Z",
     "start_time": "2020-06-29T13:40:34.881539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving url:  https://www.indeed.co.uk/jobs?q=Finance&l=London&radius=100&start=0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data argument can't be an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-844b0deb73af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data argument can't be an iterator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data argument can't be an iterator"
     ]
    }
   ],
   "source": [
    "max_jobs_to_fetch = 300\n",
    "columns = ['location', 'company_name', 'job_title', 'salary', 'summary', 'full_info']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "for start in range(0, max_jobs_to_fetch, 10):\n",
    "    try:\n",
    "#         url = 'https://www.indeed.co.uk/jobs?q=finance&l=London&radius=25&start=' + str(start)\n",
    "        url = 'https://www.indeed.co.uk/jobs?q=Finance&l=London&radius=100&start=' + str(start)\n",
    "        page = requests.get(url)\n",
    "        print('retriving url: ', url)\n",
    "    except:\n",
    "        print('---Failed to retrieve---')\n",
    "        print('url:', url)\n",
    "        continue\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    ## metadata from mainpage\n",
    "    \n",
    "    # extract info from class:row\n",
    "    # company name and job title\n",
    "    companies, jobs = [], []\n",
    "    for div in soup.find_all(name = 'div', attrs = {'class':'row'}):\n",
    "        company = div.find_all(name = 'span', attrs = {'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name = 'span', attrs = {'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    companies.append(span.text.strip())\n",
    "        for a in div.find_all(name = 'a', attrs = {'data-tn-element':\"jobTitle\"}):\n",
    "            jobs.append(a['title'])\n",
    "            \n",
    "    # extract location\n",
    "    locations = []\n",
    "    spans = soup.find_all('span', attrs={'class':'location'})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "        \n",
    "    # extract summaries\n",
    "    summaries = []\n",
    "    divs = soup.find_all('div', attrs = {'class':'summary'})\n",
    "    for i, div in enumerate(divs):\n",
    "        summaries.append(div.text.strip())\n",
    "    \n",
    "#     ## crawl to subpages\n",
    "#     descriptions = []\n",
    "#     for adlink in soup.select('a[onmousedown*=\"return rclk(this,jobmap[\"]'):\n",
    "#         suburl = \"https://www.indeed.com\" + adlink['href']\n",
    "#         try:\n",
    "#             subpage = requests.get(suburl)\n",
    "#             subsoup = BeautifulSoup(subpage.text)\n",
    "#         except:\n",
    "#             print('--- Failed to retrieve sub-URL ---')\n",
    "#             print('url: ', suburl)\n",
    "#         # extract descriptions\n",
    "#         for des in subsoup.select('div[class*=\"jobsearch-JobComponent-description\"]'): \n",
    "#              descriptions.append(des.get_text())\n",
    "            \n",
    "    df_temp = zip(locations, companies, jobs, summaries, descriptions)\n",
    "    df_temp = pd.DataFrame(df_temp, columns = columns)\n",
    "    df = df.append(df_temp).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T17:11:05.571993Z",
     "start_time": "2020-05-09T17:11:05.488337Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('sample_full.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
